import { createAnthropic } from "@ai-sdk/anthropic";
import { createGoogleGenerativeAI } from "@ai-sdk/google";
import { createMistral } from "@ai-sdk/mistral";
import { createGroq } from "@ai-sdk/groq";
import { createOpenAI } from "@ai-sdk/openai";
import { bedrock } from "@ai-sdk/amazon-bedrock";
import {
    generateObject as aiGenerateObject,
    generateText as aiGenerateText,
    type CoreTool,
    type GenerateObjectResult,
    type StepResult as AIStepResult,
    // type MessageContent as AIMessageContent,
    // type MessageContentPart
} from "ai";
import { Buffer } from 'node:buffer';
import { createOllama } from "ollama-ai-provider";
import OpenAI from 'openai';
import { encodingForModel, type TiktokenModel } from "js-tiktoken";
// import { AutoTokenizer } from "@huggingface/transformers";
import Together from "together-ai";
import type { ZodSchema } from 'zod';
import { elizaLogger } from './internal/logger-internal.js';
import {
    models,
    getModelSettings,
    // getImageModelSettings, // Remove or correct this line
    getEndpoint,
} from './models.js';
import {
    parseBooleanFromText,
    parseJsonArrayFromText,
    parseJSONObjectFromText,
    parseShouldRespondFromText,
    parseActionResponseFromText,
} from './parsing.js';
import settings from './settings.js';
import {
    type Content,
    type IAgentRuntime,
    type IImageDescriptionService,
    type ITextGenerationService,
    ModelClass,
    ModelProviderName,
    ServiceType,
    type ActionResponse,
    // type IVerifiableInferenceAdapter,
    // type VerifiableInferenceOptions,
    // type VerifiableInferenceResult,
    //VerifiableInferenceProvider,
    type TelemetrySettings,
    TokenizerType,
    // type ImageGenerationResponse,
    // type ImageCaptionResponse,
    type ModelSettings,
    type ImageModelSettings,
} from './types.js';
import { fal } from "@fal-ai/client";

import BigNumber from "bignumber.js";
import { createPublicClient, http } from "viem.js";
import fs from 'node:fs';
import os from 'node:os';
import path from 'node:path';
import * as z from 'zod';
import { v4 as uuidv4 } from 'uuid';

type Tool = CoreTool<any, any>;
type StepResult = AIStepResult<any>;

// Simplify the types to avoid deep recursion
type GenerationResult = GenerateObjectResult<unknown>;

interface ProviderOptions {
    runtime: IAgentRuntime;
    provider: ModelProviderName;
    model: string;
    apiKey: string;
    schema?: ZodSchema;
    schemaName?: string;
    schemaDescription?: string;
    mode?: "auto" | "json" | "tool";
    modelOptions: ModelSettings; // Use imported ModelSettings type
    modelClass: ModelClass;
    context: string;
}

/**
 * Trims the provided text context to a specified token limit using a tokenizer model and type.
 *
 * The function dynamically determines the truncation method based on the tokenizer settings
 * provided by the runtime. If no tokenizer settings are defined, it defaults to using the
 * TikToken truncation method with the "gpt-4o" model.
 *
 * @async
 * @function trimTokens
 * @param {string} context - The text to be tokenized and trimmed.
 * @param {number} maxTokens - The maximum number of tokens allowed after truncation.
 * @param {IAgentRuntime} runtime - The runtime interface providing tokenizer settings.
 *
 * @returns {Promise<string>} A promise that resolves to the trimmed text.
 *
 * @throws {Error} Throws an error if the runtime settings are invalid or missing required fields.
 *
 * @example
 * const trimmedText = await trimTokens("This is an example text", 50, runtime);
 * console.log(trimmedText); // Output will be a truncated version of the input text.
 */
export async function trimTokens(
    text: string,
    tokenLimit: number,
    runtime: IAgentRuntime,
): Promise<string> {
    elizaLogger.warn(
        "Using default character-based truncation logic for trimTokens."
    );
    const textAsString = text;
    if (textAsString.length > tokenLimit * 4) {
        const truncatedText = textAsString.slice(-tokenLimit * 4);
        elizaLogger.warn(
            `Truncated text from ${textAsString.length} to ${truncatedText.length} characters based on estimated token count.`
        );
        return truncatedText;
    }
    return textAsString;
}

async function truncateTiktoken(
    model: TiktokenModel,
    context: string,
    maxTokens: number
) {
    try {
        const encoding = encodingForModel(model);

        // Encode the text into tokens
        const tokens = encoding.encode(context);

        // If already within limits, return unchanged
        if (tokens.length <= maxTokens) {
            return context;
        }

        // Keep the most recent tokens by slicing from the end
        const truncatedTokens = tokens.slice(-maxTokens);

        // Decode back to text - js-tiktoken decode() returns a string directly
        return encoding.decode(truncatedTokens);
    } catch (error) {
        elizaLogger.error("Error in trimTokens:", error);
        // Return truncated string if tokenization fails
        return context.slice(-maxTokens * 4); // Rough estimate of 4 chars per token
    }
}

/**
 * Get OnChain EternalAI System Prompt
 * @returns System Prompt
 */
async function getOnChainEternalAISystemPrompt(
    runtime: IAgentRuntime
): Promise<string | undefined> {
    const agentId = runtime.getSetting("ETERNALAI_AGENT_ID");
    const providerUrl = runtime.getSetting("ETERNALAI_RPC_URL");
    const contractAddress = runtime.getSetting(
        "ETERNALAI_AGENT_CONTRACT_ADDRESS"
    );
    if (agentId && providerUrl && contractAddress) {
        // get on-chain system-prompt
        const contractABI = [
            {
                inputs: [
                    {
                        internalType: "uint256",
                        name: "_agentId",
                        type: "uint256",
                    },
                ],
                name: "getAgentSystemPrompt",
                outputs: [
                    { internalType: "bytes[]", name: "", type: "bytes[]" },
                ],
                stateMutability: "view" as const, // Add 'as const' for stricter typing
                type: "function",
            },
        ];

        const publicClient = createPublicClient({
            transport: http(providerUrl),
        });

        try {
            const validAddress: `0x${string}` =
                contractAddress as `0x${string}`;
            const result = await publicClient.readContract({
                address: validAddress,
                abi: contractABI,
                functionName: "getAgentSystemPrompt",
                args: [new BigNumber(agentId)],
            }) as unknown;
            if (result) {
                const resultArray = result as unknown[];
                elizaLogger.info("on-chain system-prompt response", resultArray[0]);
                const firstResult = resultArray[0];
                if (firstResult && typeof firstResult === 'string' && firstResult.startsWith('0x')) {
                    const value = firstResult.replace("0x", "");
                    const content = Buffer.from(value, "hex").toString("utf-8");
                    elizaLogger.info("on-chain system-prompt", content);
                    return await fetchEternalAISystemPrompt(runtime, content);
                } else {
                    elizaLogger.warn("Invalid or unexpected format for on-chain system prompt result[0]:", firstResult);
                    return undefined;
                }
            } else {
                return undefined;
            }
        } catch (error: any) { // Add type annotation for caught error
            elizaLogger.error(error);
            elizaLogger.error("err", error.message); // Access message property safely
        }
    }
    return undefined;
}

/**
 * Fetch EternalAI System Prompt
 * @returns System Prompt
 */
async function fetchEternalAISystemPrompt(
    runtime: IAgentRuntime,
    content: string
): Promise<string | undefined> {
    const url = runtime.getSetting("ETERNALAI_SYSTEM_PROMPT_URL");
    const contractAddress = runtime.getSetting(
        "ETERNALAI_AGENT_CONTRACT_ADDRESS"
    );
    if (url && contractAddress && content) {
        const body = {
            systemPrompt: content,
            address: contractAddress,
        };
        elizaLogger.info("fetching eternalai system prompt...");
        try {
            const response = await fetch(url, {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    Accept: "application/json",
                },
                body: JSON.stringify(body),
            });
            if (response.status === 200) {
                const res = await response.json();
                if (res) {
                    elizaLogger.info("eternal system prompt response");
                    const systemPrompt = res.message; // Assuming the response has a 'message' field
                    return systemPrompt;
                }
            } else {
                elizaLogger.error("Failed fetching eternal system prompt", response.status);
            }
        } catch (error: any) { // Add type annotation for caught error
            elizaLogger.error("Failed fetching eternal system prompt");
            elizaLogger.error(error);
            elizaLogger.error("err", error.message); // Access message property safely
        }
    }
    return undefined;
}

/**
 * Gets the Cloudflare Gateway base URL for a specific provider if enabled
 * @param runtime The runtime environment
 * @param provider The model provider name
 * @returns The Cloudflare Gateway base URL if enabled, undefined otherwise
 */
function getCloudflareGatewayBaseURL(
    runtime: IAgentRuntime,
    provider: string
): string | undefined {
    const isCloudflareEnabled =
        runtime.getSetting("CLOUDFLARE_GW_ENABLED") === "true";
    const cloudflareAccountId = runtime.getSetting("CLOUDFLARE_AI_ACCOUNT_ID");
    const cloudflareGatewayId = runtime.getSetting("CLOUDFLARE_AI_GATEWAY_ID");

    elizaLogger.debug("Cloudflare Gateway Configuration:", {
        isEnabled: isCloudflareEnabled,
        hasAccountId: !!cloudflareAccountId,
        hasGatewayId: !!cloudflareGatewayId,
        provider: provider,
    });

    if (!isCloudflareEnabled) {
        elizaLogger.debug("Cloudflare Gateway is not enabled");
        return undefined;
    }

    if (!cloudflareAccountId) {
        elizaLogger.warn(
            "Cloudflare Gateway is enabled but CLOUDFLARE_AI_ACCOUNT_ID is not set"
        );
        return undefined;
    }

    if (!cloudflareGatewayId) {
        elizaLogger.warn(
            "Cloudflare Gateway is enabled but CLOUDFLARE_AI_GATEWAY_ID is not set"
        );
        return undefined;
    }

    const baseURL = `https://gateway.ai.cloudflare.com/v1/${cloudflareAccountId}/${cloudflareGatewayId}/${provider.toLowerCase()}`;
    elizaLogger.info("Using Cloudflare Gateway:", {
        provider,
        baseURL,
        accountId: cloudflareAccountId,
        gatewayId: cloudflareGatewayId,
    });

    return baseURL;
}

/**
 * Send a message to the model for a text generateText - receive a string back and parse how you'd like
 * @param opts - The options for the generateText request.
 * @param opts.context The context of the message to be completed.
 * @param opts.stop A list of strings to stop the generateText at.
 * @param opts.model The model to use for generateText.
 * @param opts.frequency_penalty The frequency penalty to apply to the generateText.
 * @param opts.presence_penalty The presence penalty to apply to the generateText.
 * @param opts.temperature The temperature to apply to the generateText.
 * @param opts.max_context_length The maximum length of the context to apply to the generateText.
 * @returns The completed message.
 */

export async function generateText({
    runtime,
    context,
    modelClass,
    tools = {},
    onStepFinish,
    maxSteps = 1,
    stop,
    customSystemPrompt,
}: // verifiableInference = process.env.VERIFIABLE_INFERENCE_ENABLED === "true",
    // verifiableInferenceOptions,
    {
        runtime: IAgentRuntime;
        context: string;
        modelClass: ModelClass;
        tools?: Record<string, Tool>;
        onStepFinish?: (event: StepResult) => Promise<void> | void;
        maxSteps?: number;
        stop?: string[];
        customSystemPrompt?: string;
        // verifiableInference?: boolean;
        // verifiableInferenceAdapter?: IVerifiableInferenceAdapter;
        // verifiableInferenceOptions?: VerifiableInferenceOptions;
    }): Promise<string> {
    elizaLogger.debug(`[generateText] invoked`);
    const provider = runtime.getSetting("MODEL_PROVIDER") as ModelProviderName | null;
    const model = runtime.getSetting("MODEL_NAME") as string | null;
    const apiKey = runtime.getSetting("API_KEY") as string | null;

    if (!provider || !model || !apiKey) {
        throw new Error(
            "Missing required settings: MODEL_PROVIDER, MODEL_NAME, API_KEY"
        );
    }

    const modelSettings = getModelSettings(provider, modelClass);

    if (!modelSettings) {
        throw new Error(`Could not find model settings for provider ${provider} and class ${modelClass}`);
    }
    const { temperature, maxOutputTokens: maxTokens, frequency_penalty: frequencyPenalty, presence_penalty: presencePenalty, experimental_telemetry } =
        modelSettings;

    const systemPrompt =
        customSystemPrompt ||
        (await getOnChainEternalAISystemPrompt(runtime)) ||
        runtime.getSetting("SYSTEM_PROMPT") ||
        "You are a helpful AI assistant.";

    elizaLogger.debug(
        `[generateText] using system prompt: ${systemPrompt.substring(0, 100)}...`
    );
    elizaLogger.debug(`[generateText] using model: ${model}`);
    elizaLogger.debug(`[generateText] using modelClass: ${modelClass}`);
    elizaLogger.debug(`[generateText] context length: ${context.length}`);

    let providerInstance;

    const baseUrl = getCloudflareGatewayBaseURL(runtime, provider);

    switch (provider) {
        case ModelProviderName.ANTHROPIC:
            providerInstance = createAnthropic({ apiKey });
            break;
        case ModelProviderName.GOOGLE:
            providerInstance = createGoogleGenerativeAI({ apiKey });
            break;
        case ModelProviderName.MISTRAL:
            providerInstance = createMistral({ apiKey: apiKey });
            break;
        case ModelProviderName.GROQ:
            providerInstance = createGroq({ apiKey });
            break;
        case ModelProviderName.OPENAI:
            providerInstance = createOpenAI({
                apiKey,
                baseURL: baseUrl || undefined, // Use baseUrl if defined
            });
            break;
        case ModelProviderName.OLLAMA:
            providerInstance = createOllama({
                baseURL:
                    runtime.getSetting("OLLAMA_BASE_URL") ||
                    "http://localhost:11434/api",
            });
            break;
        // Add other providers as needed
        default:
            throw new Error(`Unsupported provider: ${provider}`);
    }

    try {
        const result = await aiGenerateText({
            model: providerInstance(model),
            system: systemPrompt,
            prompt: context,
            temperature,
            maxTokens,
            frequencyPenalty,
            presencePenalty,
            tools,
            stopSequences: stop,
            experimental_telemetry: experimental_telemetry, // Pass telemetry settings
        });

        elizaLogger.debug(
            `[generateText] result: ${result.text.substring(0, 100)}...`
        );
        return result.text;
    } catch (error: any) { // Add type annotation for caught error
        elizaLogger.error(`[generateText] Error: ${error.message}`, error);
        throw error;
    }
}

/**
 * Sends a message to the model to determine if it should respond to the given context.
 * @param opts - The options for the generateText request
 * @param opts.context The context to evaluate for response
 * @param opts.stop A list of strings to stop the generateText at
 * @param opts.model The model to use for generateText
 * @param opts.frequency_penalty The frequency penalty to apply (0.0 to 2.0)
 * @param opts.presence_penalty The presence penalty to apply (0.0 to 2.0)
 * @param opts.temperature The temperature to control randomness (0.0 to 2.0)
 * @param opts.serverUrl The URL of the API server
 * @param opts.max_context_length Maximum allowed context length in tokens
 * @param opts.max_response_length Maximum allowed response length in tokens
 * @returns Promise resolving to "RESPOND", "IGNORE", "STOP" or null
 */
export async function generateShouldRespond({
    runtime,
    context,
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<"RESPOND" | "IGNORE" | "STOP" | null> {
    const schema = z.object({
        shouldRespond: z.enum(["RESPOND", "IGNORE", "STOP"]),
    });

    try {
        const response = await generateObject({
            runtime,
            context,
            modelClass,
            schema,
            schemaName: "shouldRespond",
            schemaDescription:
                "Determine if the agent should respond to the user's message.",
            mode: "json",
        });

        // Assuming response.object is the parsed Zod object
        const parsedObject = response.object as { shouldRespond: "RESPOND" | "IGNORE" | "STOP" };
        return parsedObject?.shouldRespond ?? null;

    } catch (error: any) { // Add type annotation for caught error
        elizaLogger.error("Error generating shouldRespond object:", error.message);
        // Fallback: attempt to parse directly from text
        try {
            const textResponse = await generateText({ runtime, context, modelClass });
            return parseShouldRespondFromText(textResponse);
        } catch (textError: any) { // Add type annotation for caught error
            elizaLogger.error("Error generating fallback text for shouldRespond:", textError.message);
            return null; // Indicate failure
        }
    }
}

/**
 * Splits content into chunks of specified size with optional overlapping bleed sections
 * @param content - The text content to split into chunks
 * @param chunkSize - The maximum size of each chunk in tokens
 * @param bleed - Number of characters to overlap between chunks (default: 100)
 * @returns Promise resolving to array of text chunks with bleed sections
 */
export async function splitChunks(
    content: string,
    chunkSize = 1500,
    bleed = 100
): Promise<string[]> {
    if (!content) return [];
    // Ensure chunkSize is at least as large as bleed
    chunkSize = Math.max(chunkSize, bleed);

    const chunks: string[] = [];
    let startIndex = 0;

    while (startIndex < content.length) {
        const endIndex = Math.min(startIndex + chunkSize, content.length);
        chunks.push(content.substring(startIndex, endIndex));

        // Move to the next chunk start index, considering the bleed
        startIndex += chunkSize - bleed;
        // Ensure startIndex doesn't go backward if bleed > chunkSize (handled by Math.max above)
        // Ensure startIndex doesn't repeatedly process the same small final segment
        if (endIndex === content.length) {
            break; // Exit if we've reached the end
        }
    }

    return chunks;
}

export function splitText(
    content: string,
    chunkSize: number,
    bleed: number
): string[] {
    if (!content) return [];
    chunkSize = Math.max(chunkSize, bleed, 1); // Ensure positive chunk and bleed size
    bleed = Math.max(0, bleed); // Ensure non-negative bleed

    const chunks: string[] = [];
    let currentPos = 0;

    while (currentPos < content.length) {
        const endPos = Math.min(currentPos + chunkSize, content.length);
        chunks.push(content.slice(currentPos, endPos));
        currentPos += chunkSize - bleed;
        if (currentPos + bleed >= content.length && endPos === content.length) {
            // Avoid infinite loop on last chunk if bleed makes step size zero or negative relative to end
            break;
        }
        // Prevent potential infinite loop if chunksize <= bleed by ensuring progress
        if (chunkSize <= bleed && currentPos <= chunks[chunks.length - 1].length - chunkSize) {
            currentPos = chunks[chunks.length - 1].length - chunkSize + 1;
        }
    }

    return chunks;
}

/**
 * Sends a message to the model and parses the response as a boolean value
 * @param opts - The options for the generateText request
 * @param opts.context The context to evaluate for the boolean response
 * @param opts.stop A list of strings to stop the generateText at
 * @param opts.model The model to use for generateText
 * @param opts.frequency_penalty The frequency penalty to apply (0.0 to 2.0)
 * @param opts.presence_penalty The presence penalty to apply (0.0 to 2.0)
 * @param opts.temperature The temperature to control randomness (0.0 to 2.0)
 * @param opts.serverUrl The URL of the API server
 * @param opts.max_context_length Maximum allowed context length in tokens
 * @param opts.max_response_length Maximum allowed response length in tokens
 * @returns Promise resolving to a boolean value parsed from the model's response
 */
export async function generateTrueOrFalse({
    runtime,
    context = "",
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<boolean> {
    const schema = z.object({
        result: z.boolean(),
    });

    try {
        const response = await generateObject({
            runtime,
            context,
            modelClass,
            schema,
            schemaName: "trueOrFalse",
            schemaDescription: "Determine if the statement is true or false.",
            mode: "json",
        });
        // Assuming response.object is the parsed Zod object
        const parsedObject = response.object as { result: boolean };
        return parsedObject?.result ?? false; // Default to false on error/undefined

    } catch (error: any) { // Add type annotation for caught error
        elizaLogger.error("Error generating true/false object:", error.message);
        // Fallback: attempt to parse directly from text
        try {
            const textResponse = await generateText({ runtime, context, modelClass });
            return parseBooleanFromText(textResponse) ?? false;
        } catch (textError: any) { // Add type annotation for caught error
            elizaLogger.error("Error generating fallback text for true/false:", textError.message);
            return false; // Default to false on error
        }
    }
}

/**
 * Send a message to the model and parse the response as a string array
 * @param opts - The options for the generateText request
 * @param opts.context The context/prompt to send to the model
 * @param opts.stop Array of strings that will stop the model's generation if encountered
 * @param opts.model The language model to use
 * @param opts.frequency_penalty The frequency penalty to apply (0.0 to 2.0)
 * @param opts.presence_penalty The presence penalty to apply (0.0 to 2.0)
 * @param opts.temperature The temperature to control randomness (0.0 to 2.0)
 * @param opts.serverUrl The URL of the API server
 * @param opts.token The API token for authentication
 * @param opts.max_context_length Maximum allowed context length in tokens
 * @param opts.max_response_length Maximum allowed response length in tokens
 * @returns Promise resolving to an array of strings parsed from the model's response
 */
export async function generateTextArray({
    runtime,
    context,
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<string[]> {
    const schema = z.object({
        items: z.array(z.string()),
    });

    try {
        const response = await generateObject({
            runtime,
            context,
            modelClass,
            schema,
            schemaName: "textArray",
            schemaDescription: "Generate an array of text strings.",
            mode: "json",
        });
        // Assuming response.object is the parsed Zod object
        const parsedObject = response.object as { items: string[] };
        return parsedObject?.items ?? []; // Default to empty array

    } catch (error: any) { // Add type annotation for caught error
        elizaLogger.error("Error generating text array object:", error.message);
        // Fallback: attempt to parse directly from text
        try {
            const textResponse = await generateText({ runtime, context, modelClass });
            return parseJsonArrayFromText(textResponse) ?? [];
        } catch (textError: any) { // Add type annotation for caught error
            elizaLogger.error("Error generating fallback text for text array:", textError.message);
            return []; // Default to empty array on error
        }
    }
}

export async function generateObjectDeprecated({
    runtime,
    context,
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<any> {
    if (!context) {
        elizaLogger.error("generateObjectDeprecated context is empty");
        return null;
    }
    let retryDelay = 1000;

    while (true) {
        try {
            // this is slightly different than generateObjectArray, in that we parse object, not object array
            const response = await generateText({
                runtime,
                context,
                modelClass,
            });
            const parsedResponse = parseJSONObjectFromText(response);
            if (parsedResponse) {
                return parsedResponse;
            }
        } catch (error) {
            elizaLogger.error("Error in generateObject:", error);
        }

        await new Promise((resolve) => setTimeout(resolve, retryDelay));
        retryDelay *= 2;
    }
}

export async function generateObjectArray({
    runtime,
    context,
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<any[]> {
    if (!context) {
        elizaLogger.error("generateObjectArray context is empty");
        return [];
    }
    let retryDelay = 1000;

    while (true) {
        try {
            const response = await generateText({
                runtime,
                context,
                modelClass,
            });

            const parsedResponse = parseJsonArrayFromText(response);
            if (parsedResponse) {
                return parsedResponse;
            }
        } catch (error) {
            elizaLogger.error("Error in generateTextArray:", error);
        }

        await new Promise((resolve) => setTimeout(resolve, retryDelay));
        retryDelay *= 2;
    }
}

/**
 * Send a message to the model for generateText.
 * @param opts - The options for the generateText request.
 * @param opts.context The context of the message to be completed.
 * @param opts.stop A list of strings to stop the generateText at.
 * @param opts.model The model to use for generateText.
 * @param opts.frequency_penalty The frequency penalty to apply to the generateText.
 * @param opts.presence_penalty The presence penalty to apply to the generateText.
 * @param opts.temperature The temperature to apply to the generateText.
 * @param opts.max_context_length The maximum length of the context to apply to the generateText.
 * @returns The completed message.
 */
export async function generateMessageResponse({
    runtime,
    context,
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<Content> {
    const modelSettings = getModelSettings(runtime.modelProvider, modelClass);
    if (!modelSettings) {
        throw new Error(`Could not find model settings for provider ${runtime.modelProvider} and class ${modelClass}`);
    }
    const max_context_length = modelSettings.maxInputTokens;

    // Use string type for trimTokens input/output now
    const trimmedContext = await trimTokens(context, max_context_length, runtime);
    elizaLogger.debug("Context (trimmed string):", trimmedContext);
    context = trimmedContext; // Update context to the trimmed string

    let retryLength = 1000; // exponential backoff
    while (true) {
        try {
            elizaLogger.log("Generating message response..");

            const response = await generateText({
                runtime,
                context: context, // Use the potentially updated string context
                modelClass,
            });

            // try parsing the response as JSON, if null then try again
            const parsedContent = parseJSONObjectFromText(response) as Content;
            if (!parsedContent) {
                elizaLogger.debug("parsedContent is null, retrying");
                continue;
            }

            return parsedContent;
        } catch (error) {
            elizaLogger.error("ERROR:", error);
            // wait for 2 seconds
            retryLength *= 2;
            await new Promise((resolve) => setTimeout(resolve, retryLength));
            elizaLogger.debug("Retrying...");
        }
    }
}

export const generateImage = async (
    data: {
        prompt: string;
        width: number;
        height: number;
        count?: number;
        negativePrompt?: string;
        numIterations?: number;
        guidanceScale?: number;
        seed?: number;
        modelId?: string;
        jobId?: string;
        stylePreset?: string;
        hideWatermark?: boolean;
        safeMode?: boolean;
        cfgScale?: number;
    },
    runtime: IAgentRuntime
): Promise<any> => {
    // Use getModelSettings with ModelClass.IMAGE and cast to ImageModelSettings
    const modelSettings = getModelSettings(runtime.imageModelProvider, ModelClass.IMAGE) as ImageModelSettings | undefined;
    if (!modelSettings) {
        elizaLogger.warn(
            "No model settings found for the image model provider."
        );
        return { success: false, error: "No model settings available" };
    }
    const model = modelSettings.name;
    elizaLogger.info("Generating image with options:", {
        imageModelProvider: model,
    });

    const apiKey =
        runtime.imageModelProvider === runtime.modelProvider
            ? runtime.token
            : (() => {
                // First try to match the specific provider
                switch (runtime.imageModelProvider) {
                    case ModelProviderName.HEURIST:
                        return runtime.getSetting("HEURIST_API_KEY");
                    case ModelProviderName.TOGETHER:
                        return runtime.getSetting("TOGETHER_API_KEY");
                    case ModelProviderName.FAL:
                        return runtime.getSetting("FAL_API_KEY");
                    case ModelProviderName.OPENAI:
                        return runtime.getSetting("OPENAI_API_KEY");
                    case ModelProviderName.VENICE:
                        return runtime.getSetting("VENICE_API_KEY");
                    case ModelProviderName.LIVEPEER:
                        return runtime.getSetting("LIVEPEER_GATEWAY_URL");
                    case ModelProviderName.SECRETAI:
                        return runtime.getSetting("SECRET_AI_API_KEY");
                    case ModelProviderName.NEARAI:
                        try {
                            // Read auth config from ~/.nearai/config.json if it exists
                            const config = JSON.parse(
                                fs.readFileSync(
                                    path.join(
                                        os.homedir(),
                                        ".nearai/config.json"
                                    ),
                                    "utf8"
                                )
                            );
                            return JSON.stringify(config?.auth);
                        } catch (e) {
                            elizaLogger.warn(
                                `Error loading NEAR AI config. The environment variable NEARAI_API_KEY will be used. ${e}`
                            );
                        }
                        return runtime.getSetting("NEARAI_API_KEY");
                    default:
                        // If no specific match, try the fallback chain
                        return (
                            runtime.getSetting("HEURIST_API_KEY") ??
                            runtime.getSetting("NINETEEN_AI_API_KEY") ??
                            runtime.getSetting("TOGETHER_API_KEY") ??
                            runtime.getSetting("FAL_API_KEY") ??
                            runtime.getSetting("OPENAI_API_KEY") ??
                            runtime.getSetting("VENICE_API_KEY") ??
                            runtime.getSetting("LIVEPEER_GATEWAY_URL")
                        );
                }
            })();
    try {
        if (runtime.imageModelProvider === ModelProviderName.HEURIST) {
            const response = await fetch(
                "http://sequencer.heurist.xyz/submit_job",
                {
                    method: "POST",
                    headers: {
                        Authorization: `Bearer ${apiKey}`,
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({
                        job_id: data.jobId || crypto.randomUUID(),
                        model_input: {
                            SD: {
                                prompt: data.prompt,
                                neg_prompt: data.negativePrompt,
                                num_iterations: data.numIterations || 20,
                                width: data.width || 512,
                                height: data.height || 512,
                                guidance_scale: data.guidanceScale || 3,
                                seed: data.seed || -1,
                            },
                        },
                        model_id: model,
                        deadline: 60,
                        priority: 1,
                    }),
                }
            );

            if (!response.ok) {
                throw new Error(
                    `Heurist image generation failed: ${response.statusText}`
                );
            }

            const imageURL = await response.json();
            return { success: true, data: [imageURL] };
        } else if (
            runtime.imageModelProvider === ModelProviderName.TOGETHER ||
            // for backwards compat
            runtime.imageModelProvider === ModelProviderName.LLAMACLOUD
        ) {
            const together = new Together({ apiKey: apiKey as string });
            const response = await together.images.create({
                model: model,
                prompt: data.prompt,
                width: data.width,
                height: data.height,
                steps: (modelSettings as ImageModelSettings)?.steps ?? 4,
                n: data.count,
            });

            // Add type assertion to handle the response properly
            const togetherResponse =
                response as unknown as TogetherAIImageResponse;

            if (
                !togetherResponse.data ||
                !Array.isArray(togetherResponse.data)
            ) {
                throw new Error("Invalid response format from Together AI");
            }

            // Rest of the code remains the same...
            const base64s = await Promise.all(
                togetherResponse.data.map(async (image) => {
                    if (!image.url) {
                        elizaLogger.error("Missing URL in image data:", image);
                        throw new Error("Missing URL in Together AI response");
                    }

                    // Fetch the image from the URL
                    const imageResponse = await fetch(image.url);
                    if (!imageResponse.ok) {
                        throw new Error(
                            `Failed to fetch image: ${imageResponse.statusText}`
                        );
                    }

                    // Convert to blob and then to base64
                    const blob = await imageResponse.blob();
                    const arrayBuffer = await blob.arrayBuffer();
                    const base64 = Buffer.from(arrayBuffer).toString("base64");

                    // Return with proper MIME type
                    return `data:image/jpeg;base64,${base64}`;
                })
            );

            if (base64s.length === 0) {
                throw new Error("No images generated by Together AI");
            }

            elizaLogger.debug(`Generated ${base64s.length} images`);
            return { success: true, data: base64s };
        } else if (runtime.imageModelProvider === ModelProviderName.FAL) {
            fal.config({
                credentials: apiKey as string,
            });

            // Prepare the input parameters according to their schema
            const input = {
                prompt: data.prompt,
                image_size: "square" as const,
                num_inference_steps: (modelSettings as ImageModelSettings)?.steps ?? 50,
                guidance_scale: data.guidanceScale || 3.5,
                num_images: data.count,
                enable_safety_checker:
                    runtime.getSetting("FAL_AI_ENABLE_SAFETY_CHECKER") ===
                    "true",
                safety_tolerance: Number(
                    runtime.getSetting("FAL_AI_SAFETY_TOLERANCE") || "2"
                ),
                output_format: "png" as const,
                seed: data.seed ?? 6252023,
                ...(runtime.getSetting("FAL_AI_LORA_PATH")
                    ? {
                        loras: [
                            {
                                path: runtime.getSetting("FAL_AI_LORA_PATH"),
                                scale: 1,
                            },
                        ],
                    }
                    : {}),
            };

            // Subscribe to the model
            const result = await fal.subscribe(model, {
                input,
                logs: true,
                onQueueUpdate: (update) => {
                    if (update.status === "IN_PROGRESS") {
                        elizaLogger.info(update.logs.map((log) => log.message));
                    }
                },
            });
            // Convert the returned image URLs to base64 to match existing functionality
            const base64Promises = result.data.images.map(async (image: any) => {
                if (image.url) {
                    const response = await fetch(image.url);
                    if (!response.ok) {
                        throw new Error(
                            `Failed to fetch image: ${response.statusText}`
                        );
                    }
                    const blob = await response.blob();
                    const buffer = await blob.arrayBuffer();
                    const base64 = Buffer.from(buffer).toString("base64");
                    return `data:${image.content_type};base64,${base64}`;
                }
            });

            const base64s = await Promise.all(base64Promises);
            return { success: true, data: base64s };
        } else if (runtime.imageModelProvider === ModelProviderName.VENICE) {
            const response = await fetch(
                "https://api.venice.ai/api/v1/image/generate",
                {
                    method: "POST",
                    headers: {
                        Authorization: `Bearer ${apiKey}`,
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({
                        model: model,
                        prompt: data.prompt,
                        cfg_scale: data.guidanceScale,
                        negative_prompt: data.negativePrompt,
                        width: data.width,
                        height: data.height,
                        steps: data.numIterations,
                        safe_mode: data.safeMode,
                        seed: data.seed,
                        style_preset: data.stylePreset,
                        hide_watermark: data.hideWatermark,
                    }),
                }
            );

            const result = await response.json();

            if (!result.images || !Array.isArray(result.images)) {
                throw new Error("Invalid response format from Venice AI");
            }

            const base64s = result.images.map((base64String: string) => {
                if (!base64String) {
                    throw new Error(
                        "Empty base64 string in Venice AI response"
                    );
                }
                return `data:image/png;base64,${base64String}`;
            });

            return { success: true, data: base64s };
        } else if (
            runtime.imageModelProvider === ModelProviderName.NINETEEN_AI
        ) {
            const response = await fetch(
                "https://api.nineteen.ai/v1/text-to-image",
                {
                    method: "POST",
                    headers: {
                        Authorization: `Bearer ${apiKey}`,
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({
                        model: model,
                        prompt: data.prompt,
                        negative_prompt: data.negativePrompt,
                        width: data.width,
                        height: data.height,
                        steps: data.numIterations,
                        cfg_scale: data.guidanceScale || 3,
                    }),
                }
            );

            const result = await response.json();

            if (!result.images || !Array.isArray(result.images)) {
                throw new Error("Invalid response format from Nineteen AI");
            }

            const base64s = result.images.map((base64String: string) => {
                if (!base64String) {
                    throw new Error(
                        "Empty base64 string in Nineteen AI response"
                    );
                }
                return `data:image/png;base64,${base64String}`;
            });

            return { success: true, data: base64s };
        } else if (runtime.imageModelProvider === ModelProviderName.LIVEPEER) {
            if (!apiKey) {
                throw new Error("Livepeer Gateway is not defined");
            }
            try {
                const baseUrl = new URL(apiKey);
                if (!baseUrl.protocol.startsWith("http")) {
                    throw new Error("Invalid Livepeer Gateway URL protocol");
                }

                const response = await fetch(
                    `${baseUrl.toString()}text-to-image`,
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                            Authorization: "Bearer eliza-app-img",
                        },
                        body: JSON.stringify({
                            model_id:
                                data.modelId || "ByteDance/SDXL-Lightning",
                            prompt: data.prompt,
                            width: data.width || 1024,
                            height: data.height || 1024,
                        }),
                    }
                );
                const result = await response.json();
                if (!result.images?.length) {
                    throw new Error("No images generated");
                }
                const base64Images = await Promise.all(
                    result.images.map(async (image: any, index: number) => {
                        const filename = `${data.jobId}_${index}.png`;
                        const dir = path.join(os.tmpdir(), 'eliza-images', data.jobId || uuidv4());
                        if (!fs.existsSync(dir)) {
                            fs.mkdirSync(dir, { recursive: true });
                        }
                        const filepath = path.join(dir, filename);
                        let imageUrl;
                        if (image.url.includes("http")) {
                            imageUrl = image.url;
                        } else {
                            imageUrl = `${apiKey}${image.url}`;
                        }
                        const imageResponse = await fetch(imageUrl);
                        if (!imageResponse.ok) {
                            throw new Error(
                                `Failed to fetch image: ${imageResponse.statusText}`
                            );
                        }
                        const blob = await imageResponse.blob();
                        const arrayBuffer = await blob.arrayBuffer();
                        const base64 =
                            Buffer.from(arrayBuffer).toString("base64");
                        return `data:image/jpeg;base64,${base64}`;
                    })
                );
                return {
                    success: true,
                    data: base64Images,
                };
            } catch (error) {
                console.error(error);
                return { success: false, error: error };
            }
        } else {
            let targetSize = `${data.width}x${data.height}`;
            if (
                targetSize !== "1024x1024" &&
                targetSize !== "1792x1024" &&
                targetSize !== "1024x1792"
            ) {
                targetSize = "1024x1024";
            }
            const openaiApiKey = runtime.getSetting("OPENAI_API_KEY") as string;
            if (!openaiApiKey) {
                throw new Error("OPENAI_API_KEY is not set");
            }
            const openai = new OpenAI({
                apiKey: openaiApiKey as string,
            });
            const response = await openai.images.generate({
                model,
                prompt: data.prompt,
                size: targetSize as "1024x1024" | "1792x1024" | "1024x1792",
                n: data.count,
                response_format: "b64_json",
            });
            const imageUrls = response.data.map((img: any) => img.url || `data:image/png;base64,${img.b64_json}`);
            return {
                success: true,
                data: imageUrls,
            };
        }
    } catch (error) {
        console.error(error);
        return { success: false, error: error };
    }
};

export const generateCaption = async (
    data: { imageUrl: string },
    runtime: IAgentRuntime
): Promise<any> => {
    elizaLogger.debug(`[generateCaption] invoked for image: ${data.imageUrl}`);

    const provider = runtime.getSetting("IMAGE_CAPTION_PROVIDER") as string | null; // e.g., 'Salesforce/blip-image-captioning-large'
    const apiKey = runtime.getSetting("IMAGE_CAPTION_API_KEY") as string | null; // e.g., Hugging Face token
    const endpointUrl = getEndpoint(runtime.getSetting("IMAGE_CAPTION_PROVIDER") as ModelProviderName);

    if (!provider) {
        elizaLogger.error("Image captioning provider not configured");
        return { title: "Error", description: "Captioning provider not configured" };
    }

    // Example: Using Hugging Face Inference API
    if (endpointUrl && endpointUrl.includes("huggingface")) {
        if (!apiKey) {
            elizaLogger.error("Hugging Face API key (IMAGE_CAPTION_API_KEY) required for captioning");
            return { title: "Error", description: "API key required" };
        }
        try {
            // Fetch the image data first
            const imageResponse = await fetch(data.imageUrl);
            if (!imageResponse.ok) {
                throw new Error(`Failed to fetch image: ${imageResponse.statusText}`);
            }
            const imageBlob = await imageResponse.blob();

            const response = await fetch(endpointUrl, {
                method: "POST",
                headers: {
                    Authorization: `Bearer ${apiKey}`,
                    // Content-Type will be set automatically by fetch for Blob
                },
                body: imageBlob,
            });

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`Hugging Face API error: ${response.status} - ${errorText}`);
            }

            const result = await response.json();
            elizaLogger.debug("Hugging Face captioning result:", result);

            // Assuming the response format is [{ "generated_text": "caption" }]
            const caption = result?.[0]?.generated_text || "Could not generate caption";

            return {
                title: "Image Caption", // Or generate a title if possible
                description: caption,
            };
        } catch (error: any) { // Add type annotation
            elizaLogger.error("Error generating caption with Hugging Face:", error.message);
            return { title: "Error", description: error.message || "Caption generation failed" };
        }
    }
    // Add other providers like Google Vision, AWS Rekognition, etc. here
    else {
        elizaLogger.error(`Unsupported image captioning provider or endpoint: ${provider}`);
        return { title: "Error", description: "Unsupported captioning provider" };
    }
};

/**
 * Configuration options for generating objects with a model.
 */
export interface GenerationOptions {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
    schema?: ZodSchema;
    schemaName?: string;
    schemaDescription?: string;
    stop?: string[];
    mode?: "auto" | "json" | "tool";
    experimental_providerMetadata?: Record<string, unknown>;
    // verifiableInference?: boolean;
    // verifiableInferenceAdapter?: IVerifiableInferenceAdapter;
    // verifiableInferenceOptions?: VerifiableInferenceOptions;
}

/**
 * Generates structured objects from a prompt using specified AI models and configuration options.
 *
 * @param {GenerationOptions} options - Configuration options for generating objects.
 * @returns {Promise<any[]>} - A promise that resolves to an array of generated objects.
 * @throws {Error} - Throws an error if the provider is unsupported or if generation fails.
 */
export const generateObject = async ({
    runtime,
    context,
    modelClass,
    schema,
    schemaName,
    schemaDescription,
    stop,
    mode = "json",
}: // verifiableInference = false,
    // verifiableInferenceAdapter,
    // verifiableInferenceOptions,
    GenerationOptions): Promise<GenerateObjectResult<unknown>> => {
    elizaLogger.debug(`[generateObject] invoked`);
    const providerName = runtime.getSetting("MODEL_PROVIDER") as ModelProviderName | null;
    const modelName = runtime.getSetting("MODEL_NAME") as string | null;
    const apiKey = runtime.getSetting("API_KEY") as string | null;

    if (!providerName || !modelName || !apiKey) {
        throw new Error(
            "Missing required settings: MODEL_PROVIDER, MODEL_NAME, API_KEY"
        );
    }
    if (!schema) {
        throw new Error("generateObject requires a Zod schema.");
    }

    const modelSettings = getModelSettings(providerName, modelClass);
    const systemPrompt =
        runtime.getSetting("SYSTEM_PROMPT") || "You are a helpful AI assistant.";

    elizaLogger.debug(`[generateObject] system prompt: ${systemPrompt.substring(0, 100)}...`);
    elizaLogger.debug(`[generateObject] model: ${modelName}`);
    elizaLogger.debug(`[generateObject] modelClass: ${modelClass}`);
    elizaLogger.debug(`[generateObject] context length: ${context.length}`);
    elizaLogger.debug(`[generateObject] mode: ${mode}`);
    if (schemaName) elizaLogger.debug(`[generateObject] schemaName: ${schemaName}`);

    // --- Verifiable Inference (Optional) ---
    // if (verifiableInference) {
    //     const adapter = verifiableInferenceAdapter || getAdapter();
    //     if (!adapter) {
    //         throw new Error("Verifiable inference adapter not provided or found.");
    //     }
    //     try {
    //         const result = await adapter.generateVerifiableObject({
    //             prompt: context,
    //             systemPrompt: systemPrompt,
    //             schema: schema,
    //             options: verifiableInferenceOptions,
    //         });
    //         // TODO: Adapt VerifiableInferenceResult to GenerateObjectResult format if necessary
    //         // This might require extracting the 'object' and potentially other fields.
    //         // For now, returning a basic structure. Need to refine based on actual types.
    //         return {
    //             object: result.output,
    //             usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 }, // Placeholder usage
    //             // Add other fields like warnings, finishReason if available/mappable
    //         } as GenerateObjectResult<unknown>; // Type assertion needed
    //     } catch (error: any) {
    //         elizaLogger.error("[generateObject] Verifiable inference error:", error.message);
    //         throw error; // Rethrow or handle as needed
    //     }
    // }
    // --- End Verifiable Inference ---

    // --- Standard Generation ---
    const providerOptions: ProviderOptions = {
        runtime,
        provider: providerName,
        model: modelName,
        apiKey,
        schema,
        schemaName,
        schemaDescription,
        mode: mode as "auto" | "json" | "tool", // Cast mode safely
        modelOptions: (modelSettings ?? {}) as ModelSettings,
        modelClass,
        context, // Pass context separately as well
    };

    try {
        // Delegate to the provider-specific handler
        return await handleProvider(providerOptions);
    } catch (error: any) { // Add type annotation
        elizaLogger.error(
            `[generateObject] Error during generation with provider ${providerName}: ${error.message}`,
            error
        );
        throw error; // Re-throw the error after logging
    }
};

/**
 * Handles AI generation based on the specified provider.
 *
 * @param {ProviderOptions} options - Configuration options specific to the provider.
 * @returns {Promise<any[]>} - A promise that resolves to an array of generated objects.
 */
export async function handleProvider(
    options: ProviderOptions
): Promise<GenerationResult> {
    const {
        provider,
        runtime,
        context,
        modelClass,
        //verifiableInference,
        //verifiableInferenceAdapter,
        //verifiableInferenceOptions,
    } = options;
    const baseUrl = getCloudflareGatewayBaseURL(runtime, provider);

    // Add baseURL to options if applicable
    const updatedOptions = { ...options, apiKey: options.apiKey || '' }; // Ensure apiKey is string
    if (baseUrl && [ModelProviderName.OPENAI].includes(provider)) {
        // updatedOptions.apiKey = apiKey; // This line is redundant
        // The createOpenAI function handles baseURL internally based on provider name if not explicitly passed
        // However, passing it explicitly ensures it's used if defined.
    }

    switch (provider) {
        case ModelProviderName.ANTHROPIC:
            return handleAnthropic(updatedOptions);
        case ModelProviderName.GOOGLE:
            return handleGoogle(updatedOptions);
        case ModelProviderName.MISTRAL:
            return handleMistral(updatedOptions);
        case ModelProviderName.GROQ:
            return handleGroq(updatedOptions);
        case ModelProviderName.OLLAMA:
            return handleOllama(updatedOptions);
        case ModelProviderName.DEEPSEEK:
            return handleDeepSeek(updatedOptions);
        case ModelProviderName.BEDROCK:
            return handleBedrock(updatedOptions);
        case ModelProviderName.LIVEPEER:
            return handleLivepeer(updatedOptions);
        case ModelProviderName.NEARAI:
            return handleNearAi(updatedOptions);
        case ModelProviderName.SECRETAI:
            return handleSecretAi(updatedOptions);
        case ModelProviderName.OPENROUTER:
            return handleOpenRouter(updatedOptions);
        case ModelProviderName.REDPILL:
            return handleRedPill(updatedOptions);
        case ModelProviderName.GROK:
            return handleGrok(updatedOptions);
        default: {
            const errorMessage = `Unsupported provider: ${provider}`;
            elizaLogger.error(errorMessage);
            throw new Error(errorMessage);
        }
    }
}
/**
 * Handles object generation for OpenAI.
 *
 * @param {ProviderOptions} options - Options specific to OpenAI.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleOpenAI({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    provider,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const endpoint = runtime.character.modelEndpointOverride || getEndpoint(provider);
    const baseURL = getCloudflareGatewayBaseURL(runtime, "openai") || endpoint;
    const openai = createOpenAI({
        apiKey,
        baseURL,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: openai.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Anthropic models.
 *
 * @param {ProviderOptions} options - Options specific to Anthropic.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleAnthropic({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "auto",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    elizaLogger.debug("Handling Anthropic request with Cloudflare check");
    if (mode === "json") {
        elizaLogger.warn("Anthropic mode is set to json, changing to auto");
        mode = "auto";
    }
    const baseURL = getCloudflareGatewayBaseURL(runtime, "anthropic");
    elizaLogger.debug("Anthropic handleAnthropic baseURL:", { baseURL });

    const anthropic = createAnthropic({
        apiKey,
        baseURL,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: anthropic.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Grok models.
 *
 * @param {ProviderOptions} options - Options specific to Grok.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleGrok({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const grok = createOpenAI({
        apiKey,
        baseURL: models.grok.endpoint,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: grok.languageModel(model, { parallelToolCalls: false }),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Groq models.
 *
 * @param {ProviderOptions} options - Options specific to Groq.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleGroq({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    elizaLogger.debug("Handling Groq request with Cloudflare check");
    const baseURL = getCloudflareGatewayBaseURL(runtime, "groq");
    elizaLogger.debug("Groq handleGroq baseURL:", { baseURL });

    const groq = createGroq({
        apiKey,
        baseURL,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: groq.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Google models.
 *
 * @param {ProviderOptions} options - Options specific to Google.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleGoogle({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const google = createGoogleGenerativeAI({
        apiKey,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: google(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Mistral models.
 *
 * @param {ProviderOptions} options - Options specific to Mistral.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleMistral({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode,
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const mistral = createMistral({ apiKey: apiKey });
    return aiGenerateObject({
        model: mistral(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Redpill models.
 *
 * @param {ProviderOptions} options - Options specific to Redpill.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleRedPill({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const redPill = createOpenAI({
        apiKey,
        baseURL: models.redpill.endpoint,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: redPill.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for OpenRouter models.
 *
 * @param {ProviderOptions} options - Options specific to OpenRouter.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleOpenRouter({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const openRouter = createOpenAI({
        apiKey,
        baseURL: models.openrouter.endpoint,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: openRouter.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Ollama models.
 *
 * @param {ProviderOptions} options - Options specific to Ollama.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleOllama({
    model,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    provider,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const ollamaProvider = createOllama({
        baseURL: getEndpoint(provider) + "/api",
        fetch: runtime.fetch ?? undefined
    });
    const ollama = ollamaProvider(model);
    return aiGenerateObject({
        model: ollama,
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for DeepSeek models.
 *
 * @param {ProviderOptions} options - Options specific to DeepSeek.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleDeepSeek({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode,
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const openai = createOpenAI({
        apiKey,
        baseURL: models.deepseek.endpoint,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: openai.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Amazon Bedrock models.
 *
 * @param {ProviderOptions} options - Options specific to Amazon Bedrock.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleBedrock({
    model,
    schema,
    schemaName,
    schemaDescription,
    mode,
    modelOptions,
    provider,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const bedrockClient = bedrock(model);
    return aiGenerateObject({
        model: bedrockClient,
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

async function handleLivepeer({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode,
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    console.log("Livepeer provider api key:", apiKey);
    if (!apiKey) {
        throw new Error(
            "Livepeer provider requires LIVEPEER_GATEWAY_URL to be configured"
        );
    }

    const livepeerClient = createOpenAI({
        apiKey,
        baseURL: apiKey,
        fetch: runtime.fetch ?? undefined
    });
    return aiGenerateObject({
        model: livepeerClient.languageModel(model),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for Secret AI models.
 *
 * @param {ProviderOptions} options - Options specific to Secret AI.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleSecretAi({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    provider,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const secretAiProvider = createOllama({
        baseURL: getEndpoint(provider) + "/api",
        headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${apiKey}`,
        },
        fetch: runtime.fetch ?? undefined
    });
    const secretAi = secretAiProvider(model);
    return aiGenerateObject({
        model: secretAi,
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

/**
 * Handles object generation for NEAR AI models.
 *
 * @param {ProviderOptions} options - Options specific to NEAR AI.
 * @returns {Promise<GenerateObjectResult<unknown>>} - A promise that resolves to generated objects.
 */
async function handleNearAi({
    model,
    apiKey,
    schema,
    schemaName,
    schemaDescription,
    mode = "json",
    modelOptions,
    runtime,
}: ProviderOptions): Promise<GenerationResult> {
    const nearai = createOpenAI({
        apiKey,
        baseURL: models.nearai.endpoint,
        fetch: runtime.fetch ?? undefined
    });
    const settings = schema ? { structuredOutputs: true } : undefined;
    return aiGenerateObject({
        model: nearai.languageModel(model, settings),
        schema: schema as ZodSchema<any>,
        schemaName,
        schemaDescription,
        mode: 'json',
        ...modelOptions,
    });
}

// Add type definition for Together AI response
interface TogetherAIImageResponse {
    data: Array<{
        url: string;
        content_type?: string;
        image_type?: string;
    }>;
}

// doesn't belong here
export async function generateTweetActions({
    runtime,
    context,
    modelClass,
}: {
    runtime: IAgentRuntime;
    context: string;
    modelClass: ModelClass;
}): Promise<ActionResponse | null> {
    let retryDelay = 1000;
    while (true) {
        try {
            const response = await generateText({
                runtime,
                context,
                modelClass,
            });
            elizaLogger.debug(
                "Received response from generateText for tweet actions:",
                response
            );
            const { actions } = parseActionResponseFromText(response.trim());
            if (actions) {
                elizaLogger.debug("Parsed tweet actions:", actions);
                return actions;
            } else {
                elizaLogger.debug("generateTweetActions no valid response");
            }
        } catch (error) {
            elizaLogger.error("Error in generateTweetActions:", error);
            if (
                error instanceof TypeError &&
                error.message.includes("queueTextCompletion")
            ) {
                elizaLogger.error(
                    "TypeError: Cannot read properties of null (reading 'queueTextCompletion')"
                );
            }
        }
        elizaLogger.log(`Retrying in ${retryDelay}ms...`);
        await new Promise((resolve) => setTimeout(resolve, retryDelay));
        retryDelay *= 2;
    }
}
